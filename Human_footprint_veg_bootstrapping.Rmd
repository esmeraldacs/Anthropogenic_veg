---
title: "Untitled"
output: html_document
date: "2024-12-02"
---

#Workflow for evaluating the categorical predictive model 

```{r}
library(tidyverse)
```



```{r}
evaluate_model <- function(seed, pmp_usage_split) {
  set.seed(seed)
  
  # Split the data
  pmp_usage_sampled <- pmp_usage_split %>%
    map(~ .x %>%
          group_by(class_12km) %>%
          slice_sample(prop = 0.7) %>%
          ungroup())
  
  # Generate training data
  train_data_list <- pmp_usage_sampled %>% 
    map(~ {
      .x %>%
        pivot_longer(cols = -c(region, class_12km, ID), names_to = "name", values_to = "abundance") %>%
        mutate(abundance = abundance * 100) %>%
        group_by(region, class_12km, name) %>%
        summarise(
          mean_taxa = mean(abundance),
          sd_taxa = sd(abundance),
          .groups = "drop"
        ) %>%
        filter(mean_taxa > 0) |>
        filter(sd_taxa > 0) |>
        pivot_wider(names_from = class_12km, values_from = c(mean_taxa, sd_taxa),values_fill = 0) 
    })
  
  # Test data
  test_data_list <- map2(pmp_usage_sampled, pmp_usage_split, ~ {
    .y %>%
      anti_join(.x, by = c("ID", "region", "class_12km")) %>%
      ungroup() %>%
      pivot_longer(cols = -c(ID, region, class_12km)) %>%
      filter(value > 0) %>%
      mutate(percent = value * 100) %>%
      dplyr::select(-value) 
  })
  

  # Generate the evaluation results
  all_results <- map(names(test_data_list),~ {
    stest_df <- test_data_list[[.]]
    strain_df <- train_data_list[[.]]
    
    all <- full_join(
      stest_df,
      dplyr::select(strain_df, -region),
      by = "name"
    ) %>%
      filter(!is.na(percent)) %>%  
      filter(!is.na(mean_taxa_YES))
      
    dis_index <- all %>%
      mutate(Epsilon = 0.5) %>%
      mutate(YES_Sqrt = (percent - mean_taxa_YES)^2 / ((sd_taxa_YES + Epsilon)^2)) %>%
      mutate(NO_Sqrt = (percent - mean_taxa_NO)^2 / ((sd_taxa_NO + Epsilon)^2)) %>%
      dplyr::select(ID, class_12km, name, YES_Sqrt, NO_Sqrt) %>%
      group_by(ID) %>%
      summarise(across(c(YES_Sqrt, NO_Sqrt), sum)) %>%
      mutate(across(c(YES_Sqrt, NO_Sqrt), sqrt)) %>%
      ungroup()
    
    simi_index <- dis_index %>%
      mutate(YES = exp(-YES_Sqrt / 100)) %>%
      mutate(NO = exp(-NO_Sqrt / 100)) %>%
      dplyr::select(ID, YES, NO)
    
    sqsc <- simi_index %>%
      pivot_longer(cols = -(ID), names_to = "predicted_class") %>%
      group_by(ID) %>%
      slice_max(value) %>%
      ungroup()
    
    pred_biome <- distinct(select(stest_df, region,ID,class_12km)) %>%
      inner_join(sqsc, by = "ID") %>%
      dplyr::select(-value)
    
    observed <- ordered(pred_biome$class_12km, levels = c('YES', 'NO'))
    predicted <- ordered(pred_biome$predicted_class, levels = c('YES', 'NO'))
    
    accuracy <- mlr3measures::acc(observed, predicted) * 100
    balanced_accuracy <- mlr3measures::bacc(observed, predicted) * 100
    
    list(
      Accuracy = accuracy,
      Balanced_accuracy = balanced_accuracy,
      Region = unique(stest_df$region)
    )
  })
  
  metrics <- map_dfr(all_results, ~ data.frame(Accuracy = .x$Accuracy, 
                                               Balanced_accuracy = .x$Balanced_accuracy,
                                               Region = .x$Region))
  metrics
}
```


#Apply the function
This code is performing **n** iterations of the `evaluate_model()` function, each with a different seed (random initialization).
It then combines the results into a single dataframe, with each iteration labeled by the "iteration" column. This allows for easy comparison across different random seeds.

```{r}
# Set the seed for random number generation to ensure reproducibility of results.
# This means every time the code is run, the random processes will produce the same results.
set.seed(189)

# Generate a vector of 5 random integers between 1 and 1,000,000. 
# These will be used as different random seeds for each iteration of the model evaluation.
seeds <- sample(1:1e6, 5)

# Apply the `evaluate_model()` function to each seed in the `seeds` vector, 
# - `map_dfr()` is a purrr function that iterates over each seed, applies `evaluate_model()` to it, 
#   and combines the results into a single dataframe (`.dfr` stands for "data frame row binding").
# - The `~` syntax is used for a formula, which applies the `evaluate_model` function to each element `.x` of the `seeds` vector.
# - The `pmp_usage_split` argument is passed to `evaluate_model()` along with each seed.
# - `.id = "iteration"` adds an additional column to the resulting dataframe that indicates which iteration (or seed) the row belongs to.
results <- map_dfr(seeds, ~ evaluate_model(.x, pmp_usage_split), .id = "iteration")
```



```{r}
summary_metrics <- results %>%
  group_by(Region) %>%
  summarise(
    Mean_Accuracy = mean(Accuracy, na.rm = TRUE),
    SD_Accuracy = sd(Accuracy, na.rm = TRUE),
    Mean_Balanced_Accuracy = mean(Balanced_accuracy, na.rm = TRUE),
    SD_Balanced_Accuracy = sd(Balanced_accuracy, na.rm = TRUE)
  )
```



